{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvZYjC7bIvMW",
        "outputId": "e06c4351-d897-4198-a3ca-4c5e6ac95175",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, LSTM, Dense, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/ShakespeareModels'\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"saved_models_gpu\")\n",
        "TOKENIZER_PATH = os.path.join(MODEL_DIR, \"tokenizer.json\")\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Load dataset\n",
        "data = datasets.load_dataset('tiny_shakespeare', split='train')\n",
        "text_corpus = data['text']\n",
        "raw_text = text_corpus[0]\n",
        "\n",
        "print(f\"Dataset: {data}\")\n",
        "print(f\"First 50 characters of text: {raw_text[:50]}\")\n",
        "\n",
        "# Hyperparameter spaces\n",
        "general_params = {\n",
        "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
        "    'activation_function': ['relu', 'tanh', 'sigmoid'],\n",
        "    'dropout_rate': [0.0, 0.2, 0.5],\n",
        "    'weight_initialization': ['glorot_uniform', 'he_normal']\n",
        "}\n",
        "\n",
        "cnn_params = {\n",
        "    'filters': [32, 64, 128],\n",
        "    'kernels': [3, 5, 7],\n",
        "    'strides': [1, 2],\n",
        "    'layers': [1, 2]\n",
        "}\n",
        "\n",
        "rnn_params = {\n",
        "    'units': [64, 128, 256],\n",
        "    'layers': [1, 2, 3],\n",
        "    'cell_type': ['lstm', 'simple_rnn']\n",
        "}\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts([raw_text])\n",
        "\n",
        "with open(TOKENIZER_PATH, 'w', encoding='utf-8') as file:\n",
        "    file.write(tokenizer.to_json())\n",
        "print(f\"Tokenizer saved to: {TOKENIZER_PATH}\")\n",
        "\n",
        "encoded_text = tokenizer.texts_to_sequences([raw_text])[0]\n",
        "vocab_count = len(tokenizer.word_index) + 1\n",
        "\n",
        "seq_length = 40\n",
        "input_sequences, output_tokens = [], []\n",
        "for idx in range(len(encoded_text) - seq_length):\n",
        "    input_sequences.append(encoded_text[idx:idx + seq_length])\n",
        "    output_tokens.append(encoded_text[idx + seq_length])\n",
        "\n",
        "X_data = np.array(input_sequences)\n",
        "y_data = np.array(output_tokens)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_cnn_model(params):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_count, params['filters'][0], input_length=seq_length))\n",
        "    for i in range(params['layers']):\n",
        "        model.add(Conv1D(filters=params['filters'][i],\n",
        "                         kernel_size=params['kernels'][i],\n",
        "                         strides=params['strides'][i],\n",
        "                         activation=params['activation_function'],\n",
        "                         kernel_initializer=params['weight_initialization']))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(params['dropout_rate']))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(vocab_count, activation='softmax', kernel_initializer=params['weight_initialization']))\n",
        "    return model\n",
        "\n",
        "def create_rnn_model(params):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_count, params['units'][0], input_length=seq_length))\n",
        "    for i in range(params['layers']):\n",
        "        RNN_Cell = LSTM if params['cell_type'] == 'lstm' else SimpleRNN\n",
        "        model.add(RNN_Cell(params['units'][i],\n",
        "                           activation=params['activation_function'],\n",
        "                           return_sequences=(i < params['layers'] - 1),\n",
        "                           kernel_initializer=params['weight_initialization']))\n",
        "        model.add(Dropout(params['dropout_rate']))\n",
        "    model.add(Dense(vocab_count, activation='softmax', kernel_initializer=params['weight_initialization']))\n",
        "    return model\n",
        "\n",
        "def select_random_params(model_choice):\n",
        "    config = {key: random.choice(val) for key, val in general_params.items()}\n",
        "    if model_choice == 'cnn':\n",
        "        layers = random.choice(cnn_params['layers'])\n",
        "        config.update({\n",
        "            'filters': [random.choice(cnn_params['filters']) for _ in range(layers)],\n",
        "            'kernels': [random.choice(cnn_params['kernels']) for _ in range(layers)],\n",
        "            'strides': [random.choice(cnn_params['strides']) for _ in range(layers)],\n",
        "            'layers': layers\n",
        "        })\n",
        "    elif model_choice == 'rnn':\n",
        "        layers = random.choice(rnn_params['layers'])\n",
        "        config.update({\n",
        "            'units': [random.choice(rnn_params['units']) for _ in range(layers)],\n",
        "            'layers': layers,\n",
        "            'cell_type': random.choice(rnn_params['cell_type'])\n",
        "        })\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type.\")\n",
        "    return config\n",
        "\n",
        "trials = 5\n",
        "best_cnn_acc = -1.0\n",
        "best_cnn_config = None\n",
        "best_rnn_acc = -1.0\n",
        "best_rnn_config = None\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "print(\"--- Starting CNN Hyperparameter Search ---\")\n",
        "for run in range(trials):\n",
        "    print(f\"\\n--- CNN Trial {run+1}/{trials} ---\")\n",
        "    params = select_random_params('cnn')\n",
        "    print(\"Hyperparameters:\", params)\n",
        "\n",
        "    cnn_model = create_cnn_model(params)\n",
        "    optimizer_choice = {'adam': Adam, 'sgd': SGD, 'rmsprop': RMSprop}[params['optimizer']]\n",
        "    cnn_model.compile(optimizer=optimizer_choice(learning_rate=params['learning_rate']),\n",
        "                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = cnn_model.fit(X_train, y_train, epochs=5, batch_size=params['batch_size'],\n",
        "                            validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose=1)\n",
        "\n",
        "    val_acc = history.history['val_accuracy'][-1]\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_cnn_acc:\n",
        "        best_cnn_acc = val_acc\n",
        "        best_cnn_config = params\n",
        "        cnn_save_path = os.path.join(MODEL_DIR, f\"best_cnn_model_gpu_trial_{run+1}_val_acc_{val_acc:.4f}.keras\")\n",
        "        cnn_model.save(cnn_save_path)\n",
        "        print(f\"  --- Best CNN model saved to: {cnn_save_path}\")\n",
        "\n",
        "print(\"\\n--- CNN Hyperparameter Search Completed ---\")\n",
        "print(f\"Best CNN Validation Accuracy: {best_cnn_acc:.4f}\")\n",
        "print(\"Best CNN Hyperparameters:\", best_cnn_config)\n",
        "\n",
        "print(\"\\n--- Starting RNN Hyperparameter Search ---\")\n",
        "for run in range(trials):\n",
        "    print(f\"\\n--- RNN Trial {run+1}/{trials} ---\")\n",
        "    params = select_random_params('rnn')\n",
        "    print(\"Hyperparameters:\", params)\n",
        "\n",
        "    rnn_model = create_rnn_model(params)\n",
        "    optimizer_choice = {'adam': Adam, 'sgd': SGD, 'rmsprop': RMSprop}[params['optimizer']]\n",
        "    rnn_model.compile(optimizer=optimizer_choice(learning_rate=params['learning_rate']),\n",
        "                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = rnn_model.fit(X_train, y_train, epochs=5, batch_size=params['batch_size'],\n",
        "                             validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose=1)\n",
        "\n",
        "    val_acc = history.history['val_accuracy'][-1]\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_rnn_acc:\n",
        "        best_rnn_acc = val_acc\n",
        "        best_rnn_config = params\n",
        "        rnn_save_path = os.path.join(MODEL_DIR, f\"best_rnn_model_gpu_trial_{run+1}_val_acc_{val_acc:.4f}.keras\")\n",
        "        rnn_model.save(rnn_save_path)\n",
        "        print(f\"  --- Best RNN model saved to: {rnn_save_path}\")\n",
        "\n",
        "print(\"\\n--- RNN Hyperparameter Search Completed ---\")\n",
        "print(f\"Best RNN Validation Accuracy: {best_rnn_acc:.4f}\")\n",
        "print(\"Best RNN Hyperparameters:\", best_rnn_config)\n",
        "\n",
        "\n",
        "print(\"\\n--- Discussion ---\")\n",
        "print(\"Hyperparameter search complete using GPU. Best CNN and RNN models and tokenizer are saved in 'saved_models_gpu/' directory.\") # Changed directory name in discussion\n",
        "print(f\"Tokenizer saved to: {TOKENIZER_SAVE_PATH}\")\n",
        "print(\"Remember to evaluate these saved models on a dedicated test set for final performance assessment.\")\n",
        "print(\"Reduced trials and epochs for faster search. Increase them for a more thorough search.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wQwnTreOEuN",
        "outputId": "eee82174-61b2-4e3a-c645-f4e36e3ef3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset: Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 1\n",
            "})\n",
            "First 50 characters of text: First Citizen:\n",
            "Before we proceed any further, hear\n",
            "Tokenizer saved to: saved_models_gpu/tokenizer.json\n",
            "--- Starting CNN Hyperparameter Search ---\n",
            "\n",
            "--- CNN Trial 1/5 ---\n",
            "Hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'sgd', 'activation_function': 'tanh', 'dropout_rate': 0.2, 'weight_initialization': 'he_normal', 'num_filters': [128], 'kernel_size': [7], 'stride': [1], 'num_cnn_layers': 1}\n",
            "Epoch 1/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 4ms/step - accuracy: 0.1427 - loss: 3.2370 - val_accuracy: 0.1512 - val_loss: 3.0628\n",
            "Epoch 2/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 4ms/step - accuracy: 0.1535 - loss: 3.0567 - val_accuracy: 0.1680 - val_loss: 3.0099\n",
            "Epoch 3/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 3ms/step - accuracy: 0.1815 - loss: 2.9801 - val_accuracy: 0.1985 - val_loss: 2.8891\n",
            "Epoch 4/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 4ms/step - accuracy: 0.2036 - loss: 2.8750 - val_accuracy: 0.2135 - val_loss: 2.8128\n",
            "Epoch 5/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 3ms/step - accuracy: 0.2173 - loss: 2.8038 - val_accuracy: 0.2268 - val_loss: 2.7446\n",
            "Validation Accuracy: 0.2268\n",
            "  --- Best CNN model saved to: saved_models_gpu/best_cnn_model_gpu_trial_1_val_acc_0.2268.keras\n",
            "\n",
            "--- CNN Trial 2/5 ---\n",
            "Hyperparameters: {'learning_rate': 0.0001, 'batch_size': 128, 'optimizer': 'adam', 'activation_function': 'relu', 'dropout_rate': 0.5, 'weight_initialization': 'he_normal', 'num_filters': [32], 'kernel_size': [7], 'stride': [2], 'num_cnn_layers': 1}\n",
            "Epoch 1/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.1428 - loss: 3.1752 - val_accuracy: 0.1512 - val_loss: 3.0657\n",
            "Epoch 2/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.1534 - loss: 3.0670 - val_accuracy: 0.1513 - val_loss: 3.0512\n",
            "Epoch 3/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - accuracy: 0.1532 - loss: 3.0521 - val_accuracy: 0.1529 - val_loss: 3.0348\n",
            "Epoch 4/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.1548 - loss: 3.0377 - val_accuracy: 0.1544 - val_loss: 3.0211\n",
            "Epoch 5/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.1566 - loss: 3.0267 - val_accuracy: 0.1569 - val_loss: 3.0120\n",
            "Validation Accuracy: 0.1569\n",
            "\n",
            "--- CNN Trial 3/5 ---\n",
            "Hyperparameters: {'learning_rate': 1e-05, 'batch_size': 128, 'optimizer': 'sgd', 'activation_function': 'relu', 'dropout_rate': 0.2, 'weight_initialization': 'glorot_uniform', 'num_filters': [32], 'kernel_size': [3], 'stride': [2], 'num_cnn_layers': 1}\n",
            "Epoch 1/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.0120 - loss: 3.7173 - val_accuracy: 0.0122 - val_loss: 3.7132\n",
            "Epoch 2/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 4ms/step - accuracy: 0.0170 - loss: 3.7122 - val_accuracy: 0.0212 - val_loss: 3.7083\n",
            "Epoch 3/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - accuracy: 0.0274 - loss: 3.7071 - val_accuracy: 0.0416 - val_loss: 3.7034\n",
            "Epoch 4/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - accuracy: 0.0451 - loss: 3.7023 - val_accuracy: 0.0721 - val_loss: 3.6986\n",
            "Epoch 5/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - accuracy: 0.0697 - loss: 3.6974 - val_accuracy: 0.1035 - val_loss: 3.6939\n",
            "Validation Accuracy: 0.1035\n",
            "\n",
            "--- CNN Trial 4/5 ---\n",
            "Hyperparameters: {'learning_rate': 1e-05, 'batch_size': 64, 'optimizer': 'rmsprop', 'activation_function': 'relu', 'dropout_rate': 0.5, 'weight_initialization': 'glorot_uniform', 'num_filters': [128, 128], 'kernel_size': [7, 5], 'stride': [2, 2], 'num_cnn_layers': 2}\n",
            "Epoch 1/5\n",
            "\u001b[1m12548/12548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 4ms/step - accuracy: 0.1329 - loss: 3.3622 - val_accuracy: 0.1512 - val_loss: 3.0869\n",
            "Epoch 2/5\n",
            "\u001b[1m12548/12548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 4ms/step - accuracy: 0.1474 - loss: 3.1145 - val_accuracy: 0.1512 - val_loss: 3.0769\n",
            "Epoch 3/5\n",
            "\u001b[1m12548/12548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 4ms/step - accuracy: 0.1519 - loss: 3.0923 - val_accuracy: 0.1512 - val_loss: 3.0722\n",
            "Epoch 4/5\n",
            "\u001b[1m12548/12548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4ms/step - accuracy: 0.1527 - loss: 3.0856 - val_accuracy: 0.1512 - val_loss: 3.0708\n",
            "Validation Accuracy: 0.1512\n",
            "\n",
            "--- CNN Trial 5/5 ---\n",
            "Hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation_function': 'sigmoid', 'dropout_rate': 0.2, 'weight_initialization': 'he_normal', 'num_filters': [128], 'kernel_size': [5], 'stride': [2], 'num_cnn_layers': 1}\n",
            "Epoch 1/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 4ms/step - accuracy: 0.1445 - loss: 3.0999 - val_accuracy: 0.1683 - val_loss: 2.9906\n",
            "Epoch 2/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4ms/step - accuracy: 0.1798 - loss: 2.9620 - val_accuracy: 0.1978 - val_loss: 2.8970\n",
            "Epoch 3/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - accuracy: 0.2013 - loss: 2.8853 - val_accuracy: 0.2146 - val_loss: 2.8354\n",
            "Epoch 4/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 4ms/step - accuracy: 0.2149 - loss: 2.8363 - val_accuracy: 0.2243 - val_loss: 2.8020\n",
            "Epoch 5/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 4ms/step - accuracy: 0.2207 - loss: 2.8132 - val_accuracy: 0.2284 - val_loss: 2.7835\n",
            "Validation Accuracy: 0.2284\n",
            "  --- Best CNN model saved to: saved_models_gpu/best_cnn_model_gpu_trial_5_val_acc_0.2284.keras\n",
            "\n",
            "--- CNN Hyperparameter Search Completed ---\n",
            "Best CNN Validation Accuracy: 0.2284\n",
            "Best CNN Hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation_function': 'sigmoid', 'dropout_rate': 0.2, 'weight_initialization': 'he_normal', 'num_filters': [128], 'kernel_size': [5], 'stride': [2], 'num_cnn_layers': 1}\n",
            "\n",
            "--- Starting RNN Hyperparameter Search ---\n",
            "\n",
            "--- RNN Trial 1/5 ---\n",
            "Hyperparameters: {'learning_rate': 1e-05, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation_function': 'relu', 'dropout_rate': 0.5, 'weight_initialization': 'he_normal', 'num_neurons': [128], 'num_rnn_layers': 1, 'rnn_type': 'simple_rnn'}\n",
            "Epoch 1/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 6ms/step - accuracy: 0.1326 - loss: 3.2489 - val_accuracy: 0.2612 - val_loss: 2.7023\n",
            "Epoch 2/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 6ms/step - accuracy: 0.2584 - loss: 2.6934 - val_accuracy: 0.3060 - val_loss: 2.4687\n",
            "Epoch 3/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 6ms/step - accuracy: 0.2947 - loss: 2.5060 - val_accuracy: 0.3205 - val_loss: 2.3665\n",
            "Epoch 4/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 6ms/step - accuracy: 0.3116 - loss: 2.4182 - val_accuracy: 0.3347 - val_loss: 2.3029\n",
            "Epoch 5/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 6ms/step - accuracy: 0.3257 - loss: 2.3576 - val_accuracy: 0.3482 - val_loss: 2.2563\n",
            "Validation Accuracy: 0.3482\n",
            "  --- Best RNN model saved to: saved_models_gpu/best_rnn_model_gpu_trial_1_val_acc_0.3482.keras\n",
            "\n",
            "--- RNN Trial 2/5 ---\n",
            "Hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'activation_function': 'tanh', 'dropout_rate': 0.2, 'weight_initialization': 'glorot_uniform', 'num_neurons': [64, 128, 64], 'num_rnn_layers': 3, 'rnn_type': 'lstm'}\n",
            "Epoch 1/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 12ms/step - accuracy: 0.3299 - loss: 2.3262 - val_accuracy: 0.4816 - val_loss: 1.7100\n",
            "Epoch 2/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 12ms/step - accuracy: 0.4681 - loss: 1.7752 - val_accuracy: 0.5182 - val_loss: 1.5821\n",
            "Epoch 3/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 12ms/step - accuracy: 0.4912 - loss: 1.6849 - val_accuracy: 0.5282 - val_loss: 1.5334\n",
            "Epoch 4/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 12ms/step - accuracy: 0.5022 - loss: 1.6453 - val_accuracy: 0.5344 - val_loss: 1.5078\n",
            "Epoch 5/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 12ms/step - accuracy: 0.5085 - loss: 1.6186 - val_accuracy: 0.5401 - val_loss: 1.4900\n",
            "Validation Accuracy: 0.5401\n",
            "  --- Best RNN model saved to: saved_models_gpu/best_rnn_model_gpu_trial_2_val_acc_0.5401.keras\n",
            "\n",
            "--- RNN Trial 3/5 ---\n",
            "Hyperparameters: {'learning_rate': 0.001, 'batch_size': 128, 'optimizer': 'rmsprop', 'activation_function': 'relu', 'dropout_rate': 0.2, 'weight_initialization': 'glorot_uniform', 'num_neurons': [128, 256, 128], 'num_rnn_layers': 3, 'rnn_type': 'lstm'}\n",
            "Epoch 1/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 30ms/step - accuracy: 0.2109 - loss: 2.8182 - val_accuracy: 0.4462 - val_loss: 1.8320\n",
            "Epoch 2/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.4534 - loss: 1.8306 - val_accuracy: 0.5123 - val_loss: 1.5848\n",
            "Epoch 3/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 28ms/step - accuracy: 0.4962 - loss: 1.6690 - val_accuracy: 0.5207 - val_loss: 1.9515\n",
            "Epoch 4/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 27ms/step - accuracy: 0.5071 - loss: 1.7633 - val_accuracy: 0.5412 - val_loss: 1.4872\n",
            "Epoch 5/5\n",
            "\u001b[1m6274/6274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 28ms/step - accuracy: 0.5201 - loss: 1.5876 - val_accuracy: 0.5457 - val_loss: 1.4687\n",
            "Validation Accuracy: 0.5457\n",
            "  --- Best RNN model saved to: saved_models_gpu/best_rnn_model_gpu_trial_3_val_acc_0.5457.keras\n",
            "\n",
            "--- RNN Trial 4/5 ---\n",
            "Hyperparameters: {'learning_rate': 1e-05, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation_function': 'tanh', 'dropout_rate': 0.2, 'weight_initialization': 'he_normal', 'num_neurons': [64, 128, 256], 'num_rnn_layers': 3, 'rnn_type': 'lstm'}\n",
            "Epoch 1/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 12ms/step - accuracy: 0.1491 - loss: 3.1320 - val_accuracy: 0.1517 - val_loss: 3.0530\n",
            "Epoch 2/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 12ms/step - accuracy: 0.1551 - loss: 3.0416 - val_accuracy: 0.1668 - val_loss: 2.9998\n",
            "Epoch 3/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 12ms/step - accuracy: 0.1742 - loss: 2.9800 - val_accuracy: 0.2086 - val_loss: 2.8366\n",
            "Epoch 4/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 12ms/step - accuracy: 0.2133 - loss: 2.8245 - val_accuracy: 0.2337 - val_loss: 2.7380\n",
            "Epoch 5/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 12ms/step - accuracy: 0.2311 - loss: 2.7446 - val_accuracy: 0.2576 - val_loss: 2.6296\n",
            "Validation Accuracy: 0.2576\n",
            "\n",
            "--- RNN Trial 5/5 ---\n",
            "Hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'adam', 'activation_function': 'sigmoid', 'dropout_rate': 0.5, 'weight_initialization': 'glorot_uniform', 'num_neurons': [64, 128, 128], 'num_rnn_layers': 3, 'rnn_type': 'simple_rnn'}\n",
            "Epoch 1/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 12ms/step - accuracy: 0.1586 - loss: 3.0555 - val_accuracy: 0.2693 - val_loss: 2.4943\n",
            "Epoch 2/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 12ms/step - accuracy: 0.2610 - loss: 2.5281 - val_accuracy: 0.3090 - val_loss: 2.3553\n",
            "Epoch 3/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 11ms/step - accuracy: 0.2929 - loss: 2.4159 - val_accuracy: 0.3256 - val_loss: 2.2855\n",
            "Epoch 4/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 12ms/step - accuracy: 0.3105 - loss: 2.3566 - val_accuracy: 0.3386 - val_loss: 2.2335\n",
            "Epoch 5/5\n",
            "\u001b[1m25096/25096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 11ms/step - accuracy: 0.3207 - loss: 2.3162 - val_accuracy: 0.3440 - val_loss: 2.1933\n",
            "Validation Accuracy: 0.3440\n",
            "\n",
            "--- RNN Hyperparameter Search Completed ---\n",
            "Best RNN Validation Accuracy: 0.5457\n",
            "Best RNN Hyperparameters: {'learning_rate': 0.001, 'batch_size': 128, 'optimizer': 'rmsprop', 'activation_function': 'relu', 'dropout_rate': 0.2, 'weight_initialization': 'glorot_uniform', 'num_neurons': [128, 256, 128], 'num_rnn_layers': 3, 'rnn_type': 'lstm'}\n",
            "\n",
            "--- Best Results ---\n",
            "Best CNN Model (GPU):\n",
            "  Validation Accuracy: 0.2284\n",
            "  Hyperparameters: {'learning_rate': 0.0001, 'batch_size': 32, 'optimizer': 'rmsprop', 'activation_function': 'sigmoid', 'dropout_rate': 0.2, 'weight_initialization': 'he_normal', 'num_filters': [128], 'kernel_size': [5], 'stride': [2], 'num_cnn_layers': 1}\n",
            "\n",
            "Best RNN Model (GPU):\n",
            "  Validation Accuracy: 0.5457\n",
            "  Hyperparameters: {'learning_rate': 0.001, 'batch_size': 128, 'optimizer': 'rmsprop', 'activation_function': 'relu', 'dropout_rate': 0.2, 'weight_initialization': 'glorot_uniform', 'num_neurons': [128, 256, 128], 'num_rnn_layers': 3, 'rnn_type': 'lstm'}\n",
            "\n",
            "--- Discussion ---\n",
            "Hyperparameter search complete using GPU. Best CNN and RNN models and tokenizer are saved in 'saved_models_gpu/' directory.\n",
            "Tokenizer saved to: saved_models_gpu/tokenizer.json\n",
            "Remember to evaluate these saved models on a dedicated test set for final performance assessment.\n",
            "Reduced trials and epochs for faster search. Increase them for a more thorough search.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "def transfer_models_to_drive(target_drive_path):\n",
        "    \"\"\"\n",
        "    Transfers the 'saved_models_gpu' directory from Colab's storage\n",
        "    to a specified folder within Google Drive.\n",
        "\n",
        "    Args:\n",
        "        target_drive_path (str): Full path to the destination in Google Drive\n",
        "                                 (e.g., \"/content/drive/MyDrive/ProjectModels/saved_models_gpu\").\n",
        "                                 Ensure the desired final folder name is included.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Mount Google Drive\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\" Google Drive mounted successfully.\")\n",
        "    except Exception as error:\n",
        "        print(f\" Failed to mount Google Drive: {error}\")\n",
        "        return False\n",
        "\n",
        "    # Step 2: Define source and destination paths\n",
        "    COLAB_MODELS_DIR = \"/content/saved_models_gpu\"\n",
        "    DRIVE_DESTINATION_DIR = target_drive_path\n",
        "\n",
        "    # Verify if the source directory exists\n",
        "    if not os.path.isdir(COLAB_MODELS_DIR):\n",
        "        print(f\" Source directory '{COLAB_MODELS_DIR}' not found.\")\n",
        "        print(\"Ensure your hyperparameter search has been executed and models are saved.\")\n",
        "        return False\n",
        "\n",
        "    # Step 3: Perform the directory copy\n",
        "    try:\n",
        "        shutil.copytree(COLAB_MODELS_DIR, DRIVE_DESTINATION_DIR, dirs_exist_ok=True)\n",
        "        print(f\" Successfully copied models to '{DRIVE_DESTINATION_DIR}'.\")\n",
        "        return True\n",
        "    except Exception as error:\n",
        "        print(f\" Error while copying: {error}\")\n",
        "        print(\"Verify the destination path and check Google Drive access.\")\n",
        "        return False\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- USER CONFIGURATION ---\n",
        "    DRIVE_SAVE_PATH = \"/content/drive/MyDrive/ShakespeareExperiments/saved_models_gpu\"  # Update to your desired Google Drive folder\n",
        "    # --- END USER CONFIGURATION ---\n",
        "\n",
        "    if transfer_models_to_drive(DRIVE_SAVE_PATH):\n",
        "        print(\"\\n---  Model transfer complete. ---\")\n",
        "        print(f\"Your models are now available at: '{DRIVE_SAVE_PATH}'\")\n",
        "    else:\n",
        "        print(\"\\n---  Model transfer failed. Please review the errors above. ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuW5_VOBMffU",
        "outputId": "e7ab35e7-fff2-4e80-dcb8-b3a661dcfa1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Successfully copied '/content/saved_models_gpu' to '/content/drive/MyDrive/ShakespeareModels/saved_models_gpu' in Google Drive.\n",
            "\n",
            "--- Copying process completed. ---\n",
            "You can now find your 'saved_models_gpu' directory in Google Drive at: '/content/drive/MyDrive/ShakespeareModels/saved_models_gpu'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZ9py8563Jmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "import datasets\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def evaluate_shakespeare_models(drive_models_path):\n",
        "    \"\"\"\n",
        "    Loads and evaluates the best saved CNN and RNN models on the test split\n",
        "    of the tiny_shakespeare dataset.\n",
        "\n",
        "    Assumes both models and the tokenizer are stored in Google Drive under the provided path.\n",
        "\n",
        "    Args:\n",
        "        drive_models_path (str): Full Google Drive path to the directory containing\n",
        "                                 the models and tokenizer (e.g., \"/content/drive/MyDrive/ShakespeareModels\").\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Mount Google Drive if in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted.\")\n",
        "    except ImportError:\n",
        "        print(\"Not running inside Colab. Skipping Google Drive mount.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Setup paths\n",
        "    MODELS_DIR = os.path.join(drive_models_path, \"saved_models_gpu\")\n",
        "    TOKENIZER_PATH = os.path.join(MODELS_DIR, \"tokenizer.json\")\n",
        "    CNN_MODEL_PATH = os.path.join(MODELS_DIR, \"best_cnn_model_gpu_trial_5_val_acc_0.2284.keras\")\n",
        "    RNN_MODEL_PATH = os.path.join(MODELS_DIR, \"best_rnn_model_gpu_trial_3_val_acc_0.5457.keras\")\n",
        "    SEQ_LENGTH = 40\n",
        "\n",
        "    # Step 3: Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    try:\n",
        "        with open(TOKENIZER_PATH, 'r', encoding='utf-8') as f:\n",
        "            tokenizer = tokenizer_from_json(f.read())\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "        print(f\"Tokenizer loaded. Vocabulary size: {vocab_size}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Tokenizer not found at '{TOKENIZER_PATH}'.\")\n",
        "        return\n",
        "\n",
        "    # Step 4: Load test dataset\n",
        "    print(\"Loading test data...\")\n",
        "    test_data = datasets.load_dataset('tiny_shakespeare', split='test')\n",
        "    test_text = test_data['text'][0]\n",
        "    print(\"Test dataset loaded.\")\n",
        "\n",
        "    # Step 5: Preprocess test data\n",
        "    print(\"Preprocessing test data...\")\n",
        "    tokenized_text = tokenizer.texts_to_sequences([test_text])[0]\n",
        "\n",
        "    X_test, y_test = [], []\n",
        "    for i in range(len(tokenized_text) - SEQ_LENGTH):\n",
        "        X_test.append(tokenized_text[i:i + SEQ_LENGTH])\n",
        "        y_test.append(tokenized_text[i + SEQ_LENGTH])\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "    print(f\"Prepared {len(X_test)} test sequences.\")\n",
        "\n",
        "    # Step 6: Load models\n",
        "    print(\"Loading CNN model...\")\n",
        "    try:\n",
        "        cnn_model = tf.keras.models.load_model(CNN_MODEL_PATH)\n",
        "        print(\"CNN model loaded.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"CNN model not found at '{CNN_MODEL_PATH}'.\")\n",
        "        return\n",
        "\n",
        "    print(\"Loading RNN model...\")\n",
        "    try:\n",
        "        rnn_model = tf.keras.models.load_model(RNN_MODEL_PATH)\n",
        "        print(\"RNN model loaded.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"RNN model not found at '{RNN_MODEL_PATH}'.\")\n",
        "        return\n",
        "\n",
        "    # Step 7: Evaluate models\n",
        "    print(\"\\nEvaluating CNN model...\")\n",
        "    cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(f\"CNN Test Loss: {cnn_loss:.4f}, CNN Test Accuracy: {cnn_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluating RNN model...\")\n",
        "    rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(f\"RNN Test Loss: {rnn_loss:.4f}, RNN Test Accuracy: {rnn_accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation complete.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- USER CONFIGURATION ---\n",
        "    DRIVE_MODELS_FOLDER = \"/content/drive/MyDrive/ShakespeareModels\"\n",
        "    # --- END USER CONFIGURATION ---\n",
        "\n",
        "    evaluate_shakespeare_models(DRIVE_MODELS_FOLDER)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP6ejl1v08Dw",
        "outputId": "cea028d0-fcc0-482b-b2a4-595bac7aedc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded. Vocabulary size: 41\n",
            "Loading test dataset...\n",
            "Test dataset loaded.\n",
            "Preprocessing test data...\n",
            "Total test patterns: 55730\n",
            "Test data preprocessed.\n",
            "Loading saved CNN model...\n",
            "CNN model loaded.\n",
            "Loading saved RNN model...\n",
            "RNN model loaded.\n",
            "\n",
            "--- Evaluating CNN Model ---\n",
            "\u001b[1m1742/1742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.2252 - loss: 2.7862\n",
            "CNN Test Loss: 2.8050, CNN Test Accuracy: 0.2217\n",
            "\n",
            "--- Evaluating RNN Model ---\n",
            "\u001b[1m1742/1742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5144 - loss: 1.5774\n",
            "RNN Test Loss: 1.6520, RNN Test Accuracy: 0.5017\n",
            "\n",
            "--- Testing Completed ---\n",
            "Test results are printed above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def create_text(model, tokenizer, start_text, length=500, temp=1.0):\n",
        "    \"\"\"\n",
        "    Generates text from a trained RNN model using a given seed.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras RNN model.\n",
        "        tokenizer: Tokenizer used during training.\n",
        "        start_text: Text to start the generation from.\n",
        "        length: Number of characters to generate.\n",
        "        temp: Temperature for randomness in predictions.\n",
        "\n",
        "    Returns:\n",
        "        The generated text string.\n",
        "    \"\"\"\n",
        "\n",
        "    output_text = start_text\n",
        "    sequence = tokenizer.texts_to_sequences([start_text])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=40, padding='pre')\n",
        "\n",
        "    for _ in range(length):\n",
        "        predictions = model.predict(sequence, verbose=0)\n",
        "        next_index = pick_prediction(predictions[0], temp)\n",
        "        next_char = tokenizer.index_word.get(next_index)\n",
        "\n",
        "        if next_char is None:\n",
        "            break\n",
        "\n",
        "        output_text += next_char\n",
        "        sequence = np.append(sequence[:, 1:], [[next_index]], axis=1)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def pick_prediction(pred_probs, temp=1.0):\n",
        "    \"\"\"\n",
        "    Selects an index from the prediction probabilities using temperature.\n",
        "    \"\"\"\n",
        "    pred_probs = np.asarray(pred_probs).astype('float64')\n",
        "    pred_probs = np.log(pred_probs + 1e-8) / temp\n",
        "    exp_preds = np.exp(pred_probs)\n",
        "    prob_dist = exp_preds / np.sum(exp_preds)\n",
        "    return np.argmax(np.random.multinomial(1, prob_dist, 1))\n",
        "\n",
        "\n",
        "def load_resources(drive_path):\n",
        "    \"\"\"\n",
        "    Loads the tokenizer and the RNN model from Google Drive.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Drive mounted successfully.\")\n",
        "    except ImportError:\n",
        "        print(\"This environment is not Colab.\")\n",
        "    except Exception as err:\n",
        "        print(f\"Drive mount failed: {err}\")\n",
        "        return None, None\n",
        "\n",
        "    models_folder = os.path.join(drive_path, \"saved_models_gpu\")\n",
        "    tokenizer_file = os.path.join(models_folder, \"tokenizer.json\")\n",
        "    model_file = os.path.join(models_folder, \"best_rnn_model_gpu_trial_3_val_acc_0.5457.keras\")\n",
        "\n",
        "    print(\"Loading tokenizer...\")\n",
        "    try:\n",
        "        with open(tokenizer_file, 'r', encoding='utf-8') as file:\n",
        "            tokenizer_data = file.read()\n",
        "            tokenizer = tokenizer_from_json(tokenizer_data)\n",
        "        print(\"Tokenizer loaded.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Tokenizer not found at: {tokenizer_file}\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"Loading RNN model...\")\n",
        "    try:\n",
        "        rnn_model = tf.keras.models.load_model(model_file)\n",
        "        print(\"RNN model loaded.\")\n",
        "        return tokenizer, rnn_model\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Model file not found at: {model_file}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/ShakespeareModels\"\n",
        "    INITIAL_TEXT = \"ROMEO:\"\n",
        "    CHARACTERS_TO_GENERATE = 500\n",
        "    TEMPERATURE_VALUE = 0.8\n",
        "\n",
        "    tokenizer, model = load_resources(DRIVE_PATH)\n",
        "\n",
        "    if tokenizer and model:\n",
        "        print(f\"\\nGenerating Shakespearen Text {TEMPERATURE_VALUE}...\\n\")\n",
        "        result_text = create_text(\n",
        "            model, tokenizer, INITIAL_TEXT, CHARACTERS_TO_GENERATE, TEMPERATURE_VALUE\n",
        "        )\n",
        "        print(result_text)\n",
        "    else:\n",
        "        print(\"Tokenizer or model could not be loaded. Please check file paths.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP_JoTMr3K6z",
        "outputId": "e8748b7c-1c4d-43ab-fcd3-638f2d32e309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Loading RNN model...\n",
            "RNN model loaded.\n",
            "\n",
            "--- Generating Shakespearean Text (Temperature: 0.8) ---\n",
            "ROMEO:\n",
            "we here at my life.\n",
            "\n",
            "king richard iii:\n",
            "master man.\n",
            "\n",
            "mariana:\n",
            "when all him?\n",
            "\n",
            "remeonere:\n",
            "how thou shalt upon the mercy to feel\n",
            "shall we know, steed and fear age and but well them,\n",
            "and i can a not to well; so shall sir\n",
            "accure nor him.\n",
            "\n",
            "provost:\n",
            "'tis honed, and that have be grace of pave,\n",
            "i can more may shall for the love to it\n",
            "this your profent and forch'd upon the hold\n",
            "to stall and i know, mean, what we have being kill.\n",
            "\n",
            "menenius:\n",
            "wherefore you have our sarge-tont it;\n",
            "that defation were course th\n"
          ]
        }
      ]
    }
  ]
}